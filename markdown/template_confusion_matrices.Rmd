
```{r, include=FALSE, warning=FALSE, message=FALSE}
## Load libraries.
library(boot)
library(tidyr)
library(dplyr)
```

```{r, include=FALSE, warning=FALSE, message=FALSE}
## Import functions and variables.
source('src/confusion_matrix.R', local = confusion <- new.env())
source('src/bootstrapped_ci.R', local = bootstrapped <- new.env())
```

## Combine dataframe with measured outcome.
```{r, echo=FALSE}
discordance_status <- left_join(
  parameters$original_data %>% 
    select(
      all_of(parameters$identifier),
      all_of(parameters$model_outcome)
    ),
  parameters$aggregate_discordance_data,
  by = parameters$identifier
) %>%
  mutate(cv = parameters$cv_indicator) %>%
  relocate(cv)
```

## Calculate confusion_matrix derived measures.

- Confusion matrix:
```{r, echo=FALSE, warning=FALSE, message=FALSE}
confusion_matrix <-
  confusion$calculate_average_confusion_matrix_from_aggregate_results(
    discordance_status,
    parameters$model_prediction_class_TRUE,
    parameters$model_outcome,
    parameters$model_outcome_class_TRUE
  )

auroc <- 
  confusion$calculate_auroc_from_mean_predictions(
    discordance_status, 
    parameters$model_mean_continuous_prediction, 
    parameters$model_outcome
  )

confusion_matrix_measures <- 
  confusion$calculate_confusion_matrix_derived_measures(
    confusion_matrix, 
    parameters$model_outcome_class_TRUE
  )

confusion_matrix_measures <<-
  bind_cols(auroc, confusion_matrix_measures) %>%
  pivot_longer(cols = everything()) %>%
  rename(!!sym(parameters$cv_indicator) := value)

print(confusion_matrix)
```

- Performance characteristics with bootstrap confidence intervals:
```{r, echo=FALSE, warning=FALSE, message=FALSE}
bootstrapped_summary_performance_measures <-
  boot(discordance_status,
       confusion$get_summmary_performance_measures_as_vector,
       parameters$bootstrap_number_of_samples,
       mean_continuous_prediction = 
         parameters$model_mean_continuous_prediction,
       outcome = parameters$model_outcome, 
       outcome_class_TRUE = parameters$model_outcome_class_TRUE, 
       prediction_class_TRUE = parameters$model_prediction_class_TRUE
  )

bootstrapped_ci_performance_measures <- bootstrapped$get_bootstrapped_ci(
  bootstrapped_summary_performance_measures,
  confusion$get_variable_names_from_summmary_performance_measures_as_vector(
    discordance_status,
    parameters$model_mean_continuous_prediction, 
    parameters$model_outcome, 
    parameters$model_outcome_class_TRUE, 
    parameters$model_prediction_class_TRUE
  ), 
  parameters$bootstrap_confidence_level, 
  parameters$bootstrap_methods
)

write.csv(
  bootstrapped_ci_performance_measures,
  parameters$path_and_name_stats_with_ci,
  row.names = FALSE
)
```

```{r, echo=FALSE}
datatable(
  bootstrapped_ci_performance_measures,
  rownames = FALSE,
  extensions = list("FixedColumns" = NULL),
  options = list(
    dom = "t",
    pageLength = 20,
    scrollX = TRUE,
    fixedColumns = TRUE
  ) 
) %>%
  formatRound(
    names(bootstrapped_ci_performance_measures)[-1],
    digits = 4
  ) %>%
  formatStyle(
    names(bootstrapped_ci_performance_measures),
    lineHeight = "80%"
  )
```
