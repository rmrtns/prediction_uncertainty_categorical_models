---
title: "Mortality: Validate the use of auroc based on average continuous predictions as measure for average auroc based on individual predictions"
output: html_document
date: "2024-04-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
set.seed(seed = 200)
```

## Load libraries.
```{r, include=FALSE, warning=FALSE, message=FALSE}
library(xgboost)
library(boot)
library(pROC)
library(caret)
library(dplyr) # load after xgboost to prevent masking slice function from dplyr
library(rmarkdown)
```

## Import functions and variables.
```{r, include=FALSE, warning=FALSE, message=FALSE}
source('src/variables.R', local = variables <- new.env())
source('src/predict_mortality.R', local = mortality <- new.env())
source('src/simulate_discordance.R', local = discordance <- new.env())
source('src/confusion_matrix.R', local = confusion <- new.env())
source('src/bootstrapped_ci.R', local = bootstrapped <- new.env())
```

## Import data.
```{r}
validation_intern <- read.csv('data/selected_variables_validation_intern_db.csv')
```

## Import model.
```{r}
clf <- xgb.load('out/mortality/model/2024_01_15_11_58_xgboost.model')
```

## Simulate predictions and calculate % discordance.
```{r, echo=FALSE}
uncertain_variables <-
  variables$selected_variables[variables$uncertain_variables_indicator]
an_cv_base <-
  variables$analytical_cv_percent[variables$uncertain_variables_indicator]

test_auroc_aggregate_results <-
  discordance$simulate_variation_induced_discordance(
    validation_intern,
    "id",
    uncertain_variables,
    correlation = NA,
    an_cv_base,
    1000,
    mortality$get_continuous_prediction_mortality,
    mortality$get_categorical_prediction_mortality,
    model = clf,
    model_variables = variables$selected_variables,
    model_outcome = "X1m.mortality",
    model_cut_off = 0.18
  )

write.csv(
  test_auroc_aggregate_results,
  'out/mortality/simulations_discordance_aggregate_results/mortality_test_auroc_aggregate_results.csv',
  row.names = FALSE
)
```

## Summarise results.

- Histogram and QQ-plot continuous_prediction_mean:
```{r, echo=FALSE}
discordance$plot_distribution(
  test_auroc_aggregate_results, 
  "continuous_prediction_mean"
)
```

- Summary statistics continuous_prediction_mean
```{r, echo=FALSE}
paged_table(
  discordance$summarise_continuous_predictions_non_gaussian(
    test_auroc_aggregate_results,
    "continuous_prediction_mean"
  ) %>%
    pivot_longer(cols = everything(),
                 names_to = "continuous_prediction_mean",
                 values_to = "result")
)
```

- Histogram and QQ-plot percentage_discordant:
```{r, echo=FALSE}
discordance$plot_distribution(
  test_auroc_aggregate_results, 
  "percentage_discordant"
)
```

- Summary statistics percentage_discordant with bootstrapped confidence intervals:
```{r, echo=FALSE}
bootstrapped_summary_discordant_predictions <-
  boot(test_auroc_aggregate_results,
       discordance$get_summary_of_discordant_predictions_as_vector,
       10000)

bootstrapped_ci_discordance <- bootstrapped$get_bootstrapped_ci(
  bootstrapped_summary_discordant_predictions,
  discordance$get_variable_names_from_summary_discordant_predictions_as_vector(test_auroc_aggregate_results), 
  0.95, 
  c("norm", "perc", "bca")
)

write.csv(
  bootstrapped_ci_discordance,
  'out/mortality/simulations_discordance_aggregate_results/mortality_test_auroc_stats_with_ci.csv',
  row.names = FALSE
)
```

```{r, echo=FALSE, rows.print = 20}
paged_table(
  bootstrapped_ci_discordance %>% 
    mutate(across(where(is.numeric), \(x) round(x, 4)))
)
```

## Add outcome to dataset.
```{r, echo=FALSE}
test_auroc_aggregate_results <- left_join(validation_intern %>% select("id", "X1m.mortality"), test_auroc_aggregate_results, by = "id")
```

## Define cut-off.
cut_off = 0.18

## Reconstruct confusion matrix.
The measure of central tendency for continuous prediction that provides a good estimate of de AUROC should also provide a good estimate of the reconstructed confusion matrix with the confusion matrix based on percentage_TRUE as reference.

- Based on percentage_TRUE:
```{r, echo=FALSE}
print(
  confusion$calculate_average_confusion_matrix_from_aggregate_results(
    test_auroc_aggregate_results, 
    "percentage_TRUE", 
    "X1m.mortality", 
    "1"
  )
)
```

- Based on continuous_prediction_mean > CUT_OFF:
```{r, echo=FALSE}
print(
  confusionMatrix(
    as.factor(as.numeric(test_auroc_aggregate_results[["continuous_prediction_mean"]] > 0.18)),
    as.factor(test_auroc_aggregate_results[["X1m.mortality"]]),
    positive = '1')
)
```

- Based on continuous_prediction_median > CUT_OFF:
```{r, echo=FALSE}
print(
  confusionMatrix(
    as.factor(as.numeric(test_auroc_aggregate_results[["continuous_prediction_median"]] > 0.18)),
    as.factor(test_auroc_aggregate_results[["X1m.mortality"]]),
    positive = '1')
)
```

## Read individual results.
```{r, echo=FALSE}
list_of_files <- list.files('out/simulations_discordance_individual_results/', pattern = '.rds', recursive = TRUE, full.names = TRUE)

read_files <- function(directory){
  file <- readRDS(directory)
  outcome <- file %>% 
    select(id, simulation_index, continuous_prediction) %>%
    pivot_wider(
      names_from = simulation_index,
      values_from = continuous_prediction,
      names_prefix = "prediction_"
    )
}

list_of_individual_dbs <- lapply(list_of_files, read_files)

combined_dbs <- bind_rows(list_of_individual_dbs)

combined_dbs_with_outcome <- left_join(
  validation_intern %>% select(id, X1m.mortality),
  combined_dbs,
  by = "id")
```

## AUROC based on mean and median of AUROCs of individual simulations.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
calculate_auroc <- function(data){
  continuous_prediction <- data %>% select(-c(id, X1m.mortality))
  continuous_prediction <- as.vector(continuous_prediction)
  vector_of_auc <- vector()
  for (prediction in 1:length(continuous_prediction)){
    vector_of_auc[[prediction]] <- as.numeric(auc(roc(data[["X1m.mortality"]],
                              as.numeric(continuous_prediction[[prediction]]))))
  }
  auc_data <- data.frame(auc = vector_of_auc)
}

aurocs_per_simulation_index <- calculate_auroc(combined_dbs_with_outcome)
```

- Mean of AUROCs per simulation_index:
```{r, echo=FALSE}
print(mean(aurocs_per_simulation_index[["auc"]]))
```

- Median of AUROCs per simulation_index:
```{r, echo=FALSE}
print(median(aurocs_per_simulation_index[["auc"]]))
```

## AUROC based on continuous_prediction_mean.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
print(
  auroc_of_mean_predictions <- as.numeric(auc(roc(validation_intern[["X1m.mortality"]],
                              as.numeric(test_auroc_aggregate_results[["continuous_prediction_mean"]]))))
)
```

## AUROC based on continuous_prediction_median.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
print(auroc_of_mean_predictions <- as.numeric(auc(roc(validation_intern[["X1m.mortality"]],
                              as.numeric(test_auroc_aggregate_results[["continuous_prediction_median"]])))))
```

## Conclusion.
Results of both continuous_prediction_mean and continuous_prediction_median are similar to those based on the mean and median of the results per simulation_index, respectively. Although not numerically equal, both approaches provide a good approximation and are more computationally efficient.